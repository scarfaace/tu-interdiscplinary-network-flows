\documentclass{article}
\usepackage[utf8]{inputenc}

\title{TCP/IP communication flows into sentence-like transcriptions}
\author{Allan KÃ¡lnay}
\date{\today}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}


\begin{document}
\sloppy

\maketitle

\section*{Abstract}
The goal of this work was to design a suitable schema that transforms TCP/IP flows into sentence-like transcriptions and implement a software in Python that does such transformation from \textit{pcap} files and to evaluate its suitability for attack detection.

The schema that we implemented uses printable ASCII characters in the range from the exclamation mark character (\textit{!}) up to the tilde character (\textit{$\sim$}) which makes 93 characters altogether. Our schema utilizes four communication features to express the final transcription -- packet length, communication gaps (communication silence), communication direction, packet timestamp. With a specific configuration that we used for these four features, we were able to to convert huge pcap files (approximately 10GB) into few kB big files containing the sentence-like transcriptions of the network flows contained in those pcap files.

At the same time we were able to produce quite decent results for a classification problem distinguishing between attack and non-attack flows. For this purpose we used Random Forest Classifier on top of a basic bag of words data set constructed from the sentence-like transcriptions.

\newpage
\tableofcontents
\newpage

\section{Background}
\subsection{pcap2transcription}
pcap2transcription \cite{pcap2transcription} is a preliminary project that tries to solve the same problem as our project tries to. The project uses a similar technique for creating the transcription schema. It also deals with IP packet lengths and flow directions. Uppercase symbols describe a communication in one direction and the lower case symbols describe the communication in the other direction. The dash character (\textit{-}) stands for 10ms without packet exchange (communication gap/silence). Also, the project does not work efficiently when it comes to feature extraction as well as memory. We improved these two in our project. However, the pcap2transcription project is basically a base for our project.


\section{Methodology}
\subsection{Data}
% describe what data set I was working with (CIC2017 IDS dataset (synthetic labeled)
For the purpose of our experiments we used the synthetic CIC IDS 2017 dataset \cite{sharafaldin2018toward}. The dataset contains pcap files and labels of the network flows. The labels are binary -- attack and non-attack.

There are 5 different pcap files in this data set. Each of them represents a different day in the week from Monday to Friday. The size of all of them vary from 7GB up to 14GB.\\

\noindent\textbf{Labeling}\\

% describe how many packets do the files contain

%-------------------------------------------------------
\subsection{Data Exploration}\label{sec-dataset-exploration}
% describe what I found in data -- show the charts from the 1st and 2nd report for Felix, describe these findings and later
Since the beginning we decided to work with several network flow features -- protocol identifier, packet length, communication gaps (communication silence), communication direction, packet timestamp. Based on this decision we researched our dataset to find out more about these features and to drive our decisions later based on our findings.

The protocol identifier was purely used for identification whether a packet is a TCP/IP packet or not. If it is not a TCP/IP packet, then, as the name of the project suggests, we do not care about the packet. If it is a TCP/IP packet, we work with that.

\subsubsection{Packet Size}
As \cite{oreilly-tcp} states, the minimum size of a TCP/IP packet is 21 bytes and the maximum size of such packet is $65\ 535$ bytes. We analysed the cumulative distribution function (CDF) of packet sizes of the packets in our dataset. The output of our analysis is the figure \ref{fig-cdf-packet-sizes}. The figure provides us with information of what percentage of packets have size less than or equal to a certain value. The figure shows us that almost all of the packet sizes are less than or equal to 5kB.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{final-report/img/packet-sizes_cdf.png}
    \caption{CDF for packet sizes}
    \label{fig-cdf-packet-sizes}
\end{figure}
%-------------------------------------------------------



%-------------------------------------------------------
\subsection{Schema Description}\label{sec-schema}
Our transcription schema utilizes several network flow features. Among these are also the features that determine how output transcription schema will look like. These are:

\begin{itemize}
    \item \textit{communication direction},
    \item \textit{communication silence},
    \item \textit{packet timestamp},
    \item \textit{packet length}.
\end{itemize}

The first two are used because they are obvious and can easily help us with providing more information about the flows. The 3rd one, \textit{packet timestamp}, is used for multiple purposes. The first one is present to help our transcription maker determine when we should stop the transcription making process and cut off network flows. Once 1 minute elapses since the beginning of a network flow, we ignore this flow for the rest of the time. The second reason is that we decide the communication silences based on the time differences of two consecutive packets.

We use the \textit{packet length} feature because among the other mentioned features, this is also one that we can find in an encrypted communication as well as in a non-encrypted communication. We also used this feature as the results from the research \cite{meghdouri2018analysis} show that packet length based features are a decisive factor for identifying malicious activity.

% The output of our data processing pipeline is a tsv file with four columns -- Source IP, Destination IP, Attack, Transcription. The IP addresses represent the communication endpoints. The Attack attribute contains a binary information whether the network flow is an attack or not. The Transcription attribute contains the transcription itself.

The transcription is a string of characters describing a network flow. We use 93 characters to describe flows by transcriptions. These are printable ASCII characters in the range from the exclamation mark character (\textit{!}) up to the tilde character (\textit{$\sim$}) where each character represents a captured packet length. Each character represents different packet lengths. Transcription string characters are ordered in the same chronological order as the packet captures.

The very first character, exclamation mark, is used to represent a silence in a communication. One exclamation mark represents a silence of 1 second. The 46 characters starting from the quotation mark included up to the \textit{O} character included are used to describe one direction of a communication. The 46 characters starting from the \textit{P} character included up to the tilde character included are used to describe the other direction of a communication. The meaning of transcription characters is captured in a tabular form in the table \ref{tab:character-meanings}.

\begin{table}[h!]
\centering
\begin{tabular}{ |p{0.35\linewidth} | p{0.6\linewidth}| }
 \hline
 Character & Meaning \\
 \hline \hline
 ! & Silence in a communication of 1s time period. \\
 \hline
 " up to O & A packet sent from host A to host B. Each of these characters represents different packet length bucket. \\
 \hline
 P up to $\sim$ & A packet sent from host B to host A. Each of these characters represents different packet length bucket. \\
 \hline
\end{tabular}
\caption{Meaning of transcription characters}
\label{tab:character-meanings}
\end{table}

Based on the research in the section \ref{sec-dataset-exploration} we utilized the characters in such way that the characters represent different bins of packet lengths. We reasonably chose binning based on the analysis of the figure \ref{fig-cdf-packet-sizes}. We encoded the packets of lengths $<= 2^{14}$ with first 30 characters, the packets of lengths $(2^{14}; 2^{16}]$ with the next 15 characters and the packets of lengths $> 2^{16}$ with the last (46th) character. Naturally, each communication direction utilizes its own character set as described in the section \ref{sec-dataset-exploration}. In every category the bins are of the same size. Meaning, in the first category there are 30 bins of the same size, in the second category there are 15 bins of the same size and in the last category there is just one bin.


An example transcription may look like this:

$$ CCn!D!n $$

The meaning of the transcription character by character is the following:

\begin{enumerate}
    \item C -- host A sent a packet of length between 8752B up to 9298B
    \item C -- host A sent a packet of length between 8752B up to 9298B
    \item n -- host B sent a packet of length between 15863B up to 16384B
    \item ! -- 1s of communication silence
    \item D -- host A sent a packet of length between 9299B up to 9846B
    \item ! -- 1s of communication silence
    \item n -- host B sent a packet of length between 15863B up to 16384B
\end{enumerate}

%-------------------------------------------------------

\subsection{Output Data Set}

The output data set built from pcap files is a TSV file. The basic file structure is \textit{srcIP, dstIP, transcription}. The advanced file structure which contains also labels for network flows contains \textit{label} attribute alongside the 3 mentioned attributes. An end-user will receive a file of the basic structure if no file containing labels is specified. If such file is specified, the program tries to look up a label for each flow and assign a label and a type of attack to each entry of the output data set.

%-------------------------------------------------------


\subsection{Implementation}
% describe the software itself, the scripts for data processing, what language I used

\subsubsection{Data Processing Pipeline}
The software for processing the \textit{pcap} files and converting network flows into sentence-like transcriptions is a data processing pipeline consisted of two steps. The two-step process consists of extracting the necessary features from a \textit{pcap} file with \textit{go-flows} software \cite{goflows-reference} and the 2nd step is the transcriptions building process itself based on the output from the 1st step. The data processing pipeline itself is a shell script that orchestrates the steps to be done in order to achieve the final output. The shell pipeline is located in \verb|/transcription/pipeline.sh|.


\subsubsection{Transcriptions Maker}
As mentioned above, the 2nd step of the data processing pipeline is the transcriptions building process. This is the core software of this project implemented in Python expecting a CSV file of a certain structure as an input and producing a TSV file containing the sentence-like transcriptions as the output. Let's call this software the transcription maker.

The input CSV file for this core software is an output produced from go-flows. The configuration file used for go-flows is located in \verb|/transcription/feature_extraction/pcap2pkts.json|. This configuration file makes go-flows produce a CSV file of the following header structure: \textit{flowStartMilliseconds, protocolIdentifier, sourceIPAddress, destinationIPAddress, ipTotalLength}. Since we are dealing with packets, the \textit{flowStartMilliseconds} attribute represents a timestamp of a packet. The \textit{ipTotalLength} attribute represents a size of a IP packet itself. The other attributes are self-explaining.

Transcription maker filters the TCP packets from the input CSV file and subsequently the TCP packets get ASCII characters assigned to them based on the \textit{ipTotalLength} attribute as described in the section \ref{sec-schema}. Then, the sentence-like transcriptions of all the flows are built and outputted.

The final structure of the output TSV file is the following: \textit{srcIP, dstIP, transcription}. If a file with labels assigned to the network flows was specified, the output TSV file additionally contains one more column -- \textit{label}. Label is a binary label indicating whether a flow is attack or not.


\section{Testing}

For the testing purposes we always decided to learn Random Forest on top of bag of words for the transcription datasets comparing it to baseline Random Forest models on top of numeric vectrs representing the flows using the same features as those which were used for building transcriptions. The goal was to achieve the highest possible accuracy for the \textit{attack} class as well as for the \textit{non-attack} class. Achieving overall accuracy on top of the whole test set is meaningless for us as our data set is imbalanced and therefore overall accuracy may distort results.

We used a simple bag of words technique to express our transcription strings as numeric vectors. For each entry of the data set (a transcription) we counted number of each word in the transcription. Basically, we transformed our data set of transcriptions to the data set where our dimensions represented the word counts in transcriptions. For better understanding, see an example at figure \ref{fig-bag-of-words-example}.

\begin{figure}[h!]
\centering
\begin{tabular}{ |c|c|c|c| }
 \hline
 srcIP & dstIP & attack & transcription \\
 \hline
 1.1.1.1 & 2.2.2.2 & 1 & AAp!D!p \\
 2.2.2.2 & 3.3.3.3 & 0 & P!PD \\
 \hline
\end{tabular}

\vspace{0.3cm}

\begin{tabular}{ |c|c|c|c|c|c|c| }
 \hline
 srcIP & dstIP & A & D & P & p & ! \\
 \hline
 1.1.1.1 & 2.2.2.2 & 2 & 1 & 0 & 2 & 1 \\
 2.2.2.2 & 3.3.3.3 & 0 & 1 & 2 & 0 & 1 \\
 \hline
\end{tabular}
\caption{Data set of transcriptions (on top) and bag of words data set (bottom)}
\label{fig-bag-of-words-example}
\end{figure}


\subsection{Experiment 1}

For this experiment we used the Friday's CIC-IDS 2017 data. We were performing binary classification deciding between attack and non-attack communications. We extracted the network flows from the pcap file using go-flows. This was done using key features \textit{sourceIPAddress} and \textit{destinationIPAddress}. Next, we used \textit{active\textunderscore timeout} as well as \textit{idle\textunderscore timeout} equal to 60. This set up extracted altogether $73\ 726$ flows out of which $192$ are attack flows.

We sampled $5\ 500$ non-attack entries from this dataset and used all attacks from this dataset. This makes altogether a dataset of $5\ 692$ entries. We split both attacks and non-attacks in the 70:30 ratio of train to test set. Then we merge non-attacks with attacks for both train and test data separately in order to create a single train set and a single test set.

Learning a Random Forest model on top of bag of words created from the transcriptions and predicting the test set entries gives us very promising results as we can see at Figure \ref{fig-friday-rf-normalized} and Table \ref{tab-friday-rf-absolute}.

Even though the class imbalance was huge, we experimented with different sample weights trying to put higher weight on attack entries as there were too few of them present in the dataset, but it turned out that the best performing model is the one using the same sample weight for both.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{final-report/img/friday/friday-rf-normalized.png}
    \caption{Confusion matrix with normalized values for the experiment 1}
    \label{fig-friday-rf-normalized}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c| }
 \hline
  & true non-attack & true attack \\
 \hline
 predicted non-attack & 1635 & 15 \\
 \hline
 predicted attack & 12 & 46 \\
 \hline
\end{tabular}
\caption{Confusion matrix with absolute values for the experiment 1}
\label{tab-friday-rf-absolute}
\end{figure}

\textbf{Baseline Model}\\
TODO Spravit a popisat performance baseline modelu\\



% describe the results that I got (the confusion matrix, accuracy etc). Next describe how I constructed the train and test data set, how many entries the train and test data set contain


\subsection{Experiment 2}
This experiment has completely the same configuration as the experiment 1 with a difference of the used dataset. In the experiment 1 we worked with CIC-IDS 2017 Friday dataset. Here we are working with CIC-IDS 2017 Wednesday dataset.

The dataset contains $80\ 716$ entries out of which only 54 are attacks. We sampled $5\ 500$ non attack entries from this file for the sake of our experiment and we kept all 54 attacks. The train to test set was split in 70:30 ratio resulting in $3\ 850$ non attacks and 37 attacks in the train set and 1650 non attacks and 17 attacks in the test set.

The normalized confusion matrix in Figure \ref{fig-thursday-rf-normalized} again shows promising results. The absolute values are displayed in Table \ref{tab-thursday-rf-absolute}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{final-report/img/friday/friday-rf-normalized.png}
    \caption{Confusion matrix with normalized values for the experiment 1}
    \label{fig-thursday-rf-normalized}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c| }
 \hline
  & true non-attack & true attack \\
 \hline
 predicted non-attack & 1645 & 5 \\
 \hline
 predicted attack & 3 & 14 \\
 \hline
\end{tabular}
\caption{Confusion matrix with absolute values for the experiment 1}
\label{tab-thursday-rf-absolute}
\end{figure}




\clearpage
\section{Conclusions}
The results that we obtained are not that bad even though there is a huge room for improvement. Basically, we are able to detect around $10\%$ of attacks and at the same time misclassify only a very few non-attacks.

While building the schema we tried to avoid such network flow features that could not be used at encrypted communication. Basically all the features except of \textit{protocol identifier} are features that we can use while working with encrypted communication. The \textit{protocol identifier} feature can not be obtained in the IPSec encryption protocol. As soon as we find a way how to filter TCP/IP communication from the pcap captures without using the \textit{protocol identifier} feature, our schema can be fully used for classifying encrypted communication.



\clearpage
\bibliographystyle{plain}
\bibliography{references}


% \bibliographystyle{plain}

% \begin{thebibliography}{}

% \bibitem{cic-ids-dataset} Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani, âToward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterizationâ, 4th International Conference on Information Systems Security and Privacy (ICISSP), Portugal, January 2018


% % \bibitem{tshark-documentation} Wireshark.org. 2020. \textit{Tshark - The Wireshark Network Analyzer 3.4.0}. [online] Available at: \url{https://www.wireshark.org/docs/man-pages/tshark.html} [Accessed 28 November 2020].

% % \bibitem{python-popularity} Piatetsky, G., 2020. \textit{Python Leads The 11 Top Data Science, Machine Learning Platforms: Trends And Analysis - Kdnuggets}. [online] KDnuggets. Available at: \url{https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html} [Accessed 28 November 2020].


% \end{thebibliography}

\end{document}
